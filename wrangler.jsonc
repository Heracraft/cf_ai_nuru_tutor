{
	"$schema": "./node_modules/wrangler/config-schema.json",
	"main": ".open-next/worker.js",
	"name": "nuru-tutor",
	"compatibility_date": "2026-01-19",
	"compatibility_flags": ["nodejs_compat"],
	"services": [
		{
			"binding": "WORKFLOWS_SERVICE",
			"service": "nuru-tutor-workflows",
		},
	],
	"assets": {
		"directory": ".open-next/assets",
		"binding": "ASSETS",
	},
	"ai": {
		"binding": "AI",
	},
	"observability": {
		"logs": {
			"enabled": false,
			"invocation_logs": true,
		},
	},
	"vars": {
		// "GOOGLE_GENERATIVE_AI_API_KEY":"AIza....<my-actual-key>"
		// I think this is a good example of the importance of oversight when using LLMs.
		// Gemini 3 Pro reminded me to add var in this file, wrangler.jsonc,
		// so that the var is available inside the workers runtime
		// I caught that and immediately prompted it with "this cannot be safe" because obviously
		// we're tracking this file. I wonder how many of these 'slip ups' LLMs make on a daily
		// are 'slipping' vulnerabilities into production systems.
	},
	"d1_databases": [
		{
			"binding": "nuru_tutor_db",
			"database_name": "nuru-tutor-db",
			"database_id": "96bea162-183c-45d8-8194-cb8cd2b3af57",
			"remote": true,
		},
	],
	"env": {
		"workflows": {
			"name": "nuru-tutor-workflows",
			"main": "workflows/index.ts",
			"compatibility_date": "2025-09-27",
			"observability": {
				"enabled": true,
			},
			"workflows": [
				{
					"name": "lesson-plan-generator",
					"binding": "LESSON_PLAN_WORKFLOW",
					"class_name": "LessonPlanWorkflow",
				},
			],
			"ai": {
				"binding": "AI",
			},
			"d1_databases": [
				{
					"binding": "nuru_tutor_db",
					"database_name": "nuru-tutor-db",
					"database_id": "96bea162-183c-45d8-8194-cb8cd2b3af57",
					"remote": true,
				},
			],
		},
	},
}
